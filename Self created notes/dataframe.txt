------------- data types ------------------------------
There mainly 2 types of data types
    - numerical
        int64 - Basic python's int. It says that it uses 64 bits (8 bytes) to store int value.
        float64 - Basic python's float. It says that it uses 64 bits (8 bytes) to store double value.
    - character
        categorical - limited number of fixed values can be there in categorical column. This can save some memory for you.
        object - when number of values are not limited, object data type is used. whichever column has blank cell with NaN value, automatically gets object data type

dataTypes = cars_data.dtypes

    Price          int64
    Age          float64
    KM           float64
    FuelType      object
    HP            object
    MetColor     float64
    Automatic      int64
    CC             int64
    Doors         object
    Weight         int64

cols = cars_data.select_dtypes(include=[int])
return values of only int columns

cols = cars_data.select_dtypes(exclude=[object, int])
return values of non-int and non-object columns (only float columns)

summary =  cars_data.info()

     #   Column     Non-Null Count  Dtype
    ---  ------     --------------  -----
     0   Price      1436 non-null   int64
     1   Age        1336 non-null   float64
     2   KM         1421 non-null   float64
     3   FuelType   1336 non-null   object
     4   HP         1436 non-null   object

--------------------------------------------------------------
cars_data = pd.read_csv('Toyota.csv', index_col=0, na_values=['??', '###']) # replacing ?? and ### to NaN while loading the data from csv to dataframe.

row_ids = cars_data.index
get all index numbers (row ids)

columns = cars_data.columns
get all columns

size = cars_data.size
get total number of rows

shape = cars_data.shape
returns a tuple of (number of rows, number of columns)

memory = cars_data.memory_usage()
returns memory usage in bytes for every column

student = {
    'Name': ['John', 'Jay', 'sachin', 'Geetha', 'Amutha', 'ganesh'],
    'gender': ['male', 'male', 'male', 'female', 'female', 'male'],
    'math score': [50, 100, 70, 80, 75, 40],
    'test preparation': ['none', 'completed', 'none', 'completed',
                         'completed', 'none'],
}
df = pd.DataFrame(student)
creating dataframe from dictionary (map)


cars_data2 = cars_data.copy()
creating a copy of dataframe. Working on the copy, so that original data remains unchanged.

------------------- Filtering (head, tail, at, iat, loc, iloc, filtering through condition in df, where ----------------------------------
firstNRows = cars_data.head(15)
selecting the first 15 rows of dataframe. By default, it returns the first 5 rows.

lastNRows = cars_data.tail(15)
selecting the last 15 rows of dataframe. By default, it returns the last 5 rows.

result = cars_data.at[4, 'FuelType'] --- 4th row of FuelType column
at[row id, column name] returns a cell data

result = cars_data.iat[4, 5] --- 4th row of FuelType column
iat[row id, column id] returns a cell data

Instead of at()/iat(), you can use
cars_data['FuelType'][4]   --- 4th row of FuelType column

df.loc takes two arguments, 'row index' and 'column index'
------

data.loc[(data.Brand == 'Maruti') & (data.Mileage > 25)]
selecting rows with all columns where Brand=Maruti and Mileage>25

data.loc[(data.Brand == 'Maruti') & (data.Mileage > 25), ['Mileage']]
selecting rows with only Mileage column where Brand=Maruti and Mileage>25

result = cars_data.loc[4:15, ['Price', 'FuelType']]
returns data of specific rows and columns

data.loc[(data.Year < 2015), ['Mileage']] = 22
change the values of Mileage column where year<2015

df.loc[df['First season'] > 1990, 'First Season'] = 1

df.iloc() function is an indexed-based selecting method
---------
data.iloc[[0, 2, 4, 7]]
selecting 0th, 2nd, 4th, and 7th index rows

data.iloc[1: 5, 2: 5]
selecting rows from 1 to 4 and columns from 2 to 4

filtered_data = dataframe[dataframe['Percentage'] > 80]
selecting rows based on condition. works same as loc() function.

filtered_data = np.where(dataframe['Percentage'] > 80,
                dataframe['Percentage'],
                np.nan)
if dataframe['Percentage'] > 80, then return dataframe['Percentage'], otherwise return NaN

--------------------nulls, not nulls summary------------------------------
summary =  cars_data.info()
returns the summary of dataframe

     #   Column     Non-Null Count  Dtype
    ---  ------     --------------  -----
     0   Price      1436 non-null   int64
     1   Age        1336 non-null   float64
     2   KM         1421 non-null   float64
     3   FuelType   1336 non-null   object
     4   HP         1430 non-null   float64

Price_Summary = cars_data['Price'].info()
returns the summary of a 'Price' column
    1436 non-null   int64

NaN_cells_in_each_column = cars_data.isnull().sum()   or   cars_data.isna().sum()
returns total null(NaN) cells in each column
    Price          0
    Age          100
    KM            15
    FuelType     100
    HP             6

--------------------- Data Clean Up (read_csv with nav_values, unique, replace, where, loc functions) ------------------------
cars_data = pd.read_csv(filepath_or_buffer = 'Toyota.csv', index_col=0, na_values=["??", "????"])
replacing ?? and ???? with NaN

doors_unique_elements = np.unique(cars_data['Doors'])
find unique values

cars_data.replace(to_replace='three', value=3, inplace=True)
cars_data.replace(to_replace='four', value=4, inplace=True)
cars_data.replace(to_replace='five', value=5, inplace=True)
replace values in entire dataframe(all columns)

cars_data["Doors"] = np.where(dataframe['Doors'] = 'three',
                3,
                dataframe['Doors'])
replacing data in a particular column


cars_data['Doors'] = cars_data['Doors'].astype(dtype='int64')
replacing datatype from object to int

cars_data.loc[df['Doors'] = 'three', 'Doors'] = 3

------------------ missing data (isnull, isna, isnull().any(axis=1), df[filters], describe(), fillna()-----------------

Either you can remove missing (NaN) data or replace them with proper value.
There are two ways to fill in the missing values.
    1. If it is a numerical column, then you can fill in mean/median
    2. If it's a categorical column, then you can fill in the max occurring category

NaN_cells_in_each_column = cars_data.isnull().sum()   or   cars_data.isna().sum()
returns total null(NaN) cells in each column
    Price          0
    Age          100
    KM            15
    FuelType     100
    HP             6


rows_with_missing_values = cars_data[cars_data.isnull().any(axis=1)]
selecting rows that have one or more missing values

rows_with_missing_age = cars_data[cars_data['Age'].isnull()]
find rows with missing (NaN) age.

filter1 = rows_with_missing_values['Age'].isnull()
filter2 = rows_with_missing_values['KM'].isnull()
rows_with_missing_age_and_km = cars_data[filter1 & filter2]
finding rows with missing age and km


print(cars_data.describe())
returns 5 pointer analysis for numeric variables

    Average is represented by mean.
    Median is represented by 50%

                  Price          Age  ...  Age_Converted2  Km_per_month
    count   1436.000000  1336.000000  ...     1336.000000   1321.000000
    mean   10730.824513    55.672156  ...        4.639346      0.111520
    std     3626.964585    18.589804  ...        1.549150      2.526162
    min     4350.000000     1.000000  ...        0.083333      0.000177
    25%     8450.000000    43.000000  ...        3.583333      0.000661
    50%     9900.000000    60.000000  ...        5.000000      0.000880
    75%    11950.000000    70.000000  ...        5.833333      0.001156
    max    32500.000000    80.000000  ...        6.666667     76.000000

print(cars_data.describe(inlcude='o'))
             JobType    EdType        maritalstatus       occupation relationship    race gender   nativecountry                        SalStat
    count      31978     31978                31978            31978        31978   31978  31978           31978                          31978
    unique         9        16                    7               15            6       5      2              41                              2
    top      Private   HS-grad   Married-civ-spouse   Prof-specialty      Husband   White   Male   United-States   less than or equal to 50,000
    freq       22286     10368                14692             4038        12947   27430  21370           29170                          24283


Filling up NaN Age with mean value
    mean_of_age = cars_data2['Age'].mean()
    print(mean_of_age) # 55.67215568862275
    cars_data2.fillna({'Age' : mean_of_age}, inplace=True)


Filling up NaN Age with median value
    median_of_km = cars_data2['KM'].median()
    print(median_of_km) # 63061.5
    cars_data2.fillna({'KM' : median_of_km}, inplace=True)

Filling up NaN MetColor with mode value
    In statistics, the mode is the value that is repeatedly occurring in a given set. We can also say that the value or number in a data set, which has a high frequency or appears more frequently, is called mode or modal value.

    mode_value_of_MetColor = cars_data2['MetColor'].mode()
    cars_data2.fillna({'MetColor': mode_value_of_MetColor[0]}, inplace=True)


For Categorical Data
--------------------
crosstab with columns='count'/value_counts()/countplot/barplot are same.
They show the frequencies.

fuel_types_counts = cars_data2['FuelType'].value_counts(ascending=False)
    FuelType
    Petrol    1177
    Diesel     144
    CNG         15

indices = fuel_types_counts.index
cars_data2.fillna({'FuelType' : indices[0]}, inplace=True)
filling up NaN FuelType with 'Petrol'


------------------ Inserting a new column and filling up the data in it ------------------
# create new column in dataframe with default value as 0
cars_data.insert(10, "Age_Converted", 0)

def age_convert(val):
    val_converted = val/12
    return round(val_converted)

cars_data["Age_Converted"] = age_convert(cars_data["Age"])

print(age_converted_series.unique())

cars_data["Age_Converted"].fillna(0)

cars_data["Age_Converted"] = cars_data["Age_Converted"].astype(dtype="int64")

OR

for i in range(0, len(cars_data["Price"]), 1):
    if (cars_data["Price"][i] <= 8450):
        value = "Low"
    elif (cars_data["Price"][i] >= 11950):
        value = "High"
    else:
        value = "Medium"
    # See indexingAndSelectingData.py to know more about df.loc() method.
    cars_data.loc[i, "Price_Class"] = value

----------------------- Series ----------------------
Series is like a column in a table. Each row in a series gets a label. Either you can assign a label or it gets its own using ids.
https://www.w3schools.com/python/pandas/pandas_series.asp
On any series, you can call value counts(). It gives you a count of all types of values in that series.
In dataframe, each column returns a Series type.

price_class_series = cars_data["Price_Class"] # column is a series
values = price_class_series.value_counts()

Price_Class
Medium    704
Low       369
High      363

index = price_class_series.index

values.get(index[0]) # 704

---------------- Correlation -----------------

Correlation shows the strength of relation between two variables.
Variables do not have to be numeric, but for our example, we will use numeric variables.

Correlation is bound between -1 and +1. 0 means no correlation at all.
# Watch 'Pearson Correlation Coefficient' video
# https://www.youtube.com/watch?v=B8LcYdblXBI

Closer to +1 represents there is a strong positive correlation between two variables.
Theoretically, above 0.7, you can say there is a fair correlation between two numerical variables.

Closer to -1 represents stronger inverse correlation between two numerical variables.
E.g. relation between price and age of the car. As car gets older, price goes down.
Relation between price and km. As car is driven more, its price goes down.
Relation between price and weight. As weight of the car increases, its price also increases. There is a positive correlation between price and weight.
Relation between Age and KM. As Age of the car increases, its KMs also increases. There is a positive correlation in between these Age and KM.

Pearson method can calculate correlation only between two numeric variables.
Pearson method is a default method.

result = cars_data_copy.corr(method="pearson", numeric_only=True)
print(result)

              Price       Age        KM  ...  Automatic        CC    Weight
Price      1.000000 -0.878407 -0.574720  ...   0.033081  0.165067  0.581198
Age       -0.878407  1.000000  0.512735  ...   0.032573 -0.120706 -0.464299
KM        -0.574720  0.512735  1.000000  ...  -0.081248  0.299993 -0.026271
MetColor   0.112041 -0.099659 -0.093825  ...  -0.013973  0.029189  0.057142
Automatic  0.033081  0.032573 -0.081248  ...   1.000000 -0.069321  0.057249
CC         0.165067 -0.120706  0.299993  ...  -0.069321  1.000000  0.651450
Weight     0.581198 -0.464299 -0.026271  ...   0.057249  0.651450  1.000000

result = cars_data_copy.corr(method="kendall", numeric_only=True)
print(result)

result = cars_data_copy.corr(method="spearman", numeric_only=True)
print(result)

----------------- Frequency Table (crosstab) -------------------
Frequency Table
---------------
it should be used mainly for categorical variables
crosstab with columns='count' is same as value_counts()/countplot/barplot

fuel_types_counts = cars_data2['FuelType'].value_counts(ascending=False)
    FuelType
    Petrol    1177
    Diesel     144
    CNG         15

result = pd.crosstab(index=cars_data_copy['FuelType'],
                     columns='count',
                     dropna=False)
   col_0     count
    FuelType
    CNG          15
    Diesel      144
    Petrol     1177
    NaN         100

Two-way frequency table
-----------------------
It finds out the FREQUENCY cross table between two variables.

    result = pd.crosstab(index=cars_data_copy['Automatic'], # index means row
                     columns=cars_data_copy['FuelType'],
                     dropna=False)

    FuelType   CNG  Diesel  Petrol  NaN
    Automatic
    0           15     144    1104   93
    1            0       0      73    7
    This relationship shows that Automatic cars (value=1) has only Petrol cars.

Two-way Joint Probability
-------------------------
By setting normalize=True in crosstab(), you can convert the values to percentage (probability).

    result = pd.crosstab(index=cars_data_copy['Automatic'],# index means row
                         columns=cars_data_copy['FuelType'],
                         normalize=True, # convert the values to percentage (probability)
                         dropna=True)
    print(result)

    FuelType        CNG    Diesel    Petrol
    Automatic
    0          0.011228  0.107784  0.826347
    1          0.000000  0.000000  0.054641

    This means the probability of Automatic cars having petrol engine is really high.

Two-Way Marginal Probability Table
----------------------------------

You will get row sums and column sums by setting margins=True
shows correlation between the data of two variables

    result = pd.crosstab(index=cars_data_copy['Automatic'],
                         columns=cars_data_copy['FuelType'],
                         normalize=True, # convert the values to percentage (probability)
                         margins=True, # Gives sums of rows and sums of columns
                         dropna=True) # do not consider NaN values

    FuelType        CNG    Diesel    Petrol       All
    Automatic
    0          0.011228  0.107784  0.826347  0.945359
    1          0.000000  0.000000  0.054641  0.054641
    All        0.011228  0.107784  0.880988  1.000000

Two-Way Conditional Property Table
----------------------------------
    https://www.youtube.com/watch?v=-bijcTgRVBQ - watch this video to understand what is conditional probability

    Conditional probability:
    focuses on the probability of one event given that another event has already occurred.

    In above data with margins,
    Conditional probability P(Petrol | Automatic) = probability of petrol with given probability of Automatic
                                                  = 0.826347/0.945359
                                                  = 0.874109

    result = pd.crosstab(index=cars_data_copy['Automatic'],
                         columns=cars_data_copy['FuelType'],
                         normalize='index', # means sum cols in a row should be 1
                         margins=True, # Gives sums of rows and sums of columns
                         dropna=True) # do not consider NaN values
    print(result)

    FuelType        CNG    Diesel    Petrol
    Automatic
    0          0.011876  0.114014  0.874109
    1          0.000000  0.000000  1.000000
    All        0.011228  0.107784  0.880988


    Conditional probability P(Automatic | Petrol) = probability of Automatic with given probability of Petrol
                                              = 0.826347/0.880988
                                              = 0.937978

    result = pd.crosstab(index=cars_data_copy['Automatic'],
                         columns=cars_data_copy['FuelType'],
                         normalize='columns', # Sum of rows in a column will become 1
                         margins=True, # Gives sums of rows and sums of columns
                         dropna=True) # do not consider NaN values
    print(result)

    FuelType   CNG  Diesel    Petrol       All
    Automatic
    0          1.0     1.0  0.937978  0.945359
    1          0.0     0.0  0.062022  0.054641

------------------------- Data visualization (plots/charts) -------------------